---
title: "Research"
layout: splash
permalink: /research/
---

I am interested in artificial and natural intelligence. Particularly, I'm focusing on how artificial general intelligence could be achieved through Reinforcement Learning. I am fortunate to be advised by professors [George Konidaris](https://cs.brown.edu/people/gdk/) and [Michael Littman](https://www.littmania.com) under Brown's [Big AI initiative](http://bigai.cs.brown.edu). Feel free to check out some of my work below. 
{: style="text-align: center;font-size:110%;padding-top:40px"}



## Publications
### Conferences 
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <!-- <tr bgcolor="#ffffd0"> -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div>
                <img src='/images/paper-images/actgen.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <p style="font-family:'Lato',Verdana,Helvetica,sans-serif; font-size:14px;font-weight:700">
              Characterizing the Action Generalization Gap in Deep Q-Learning
              </p>
              <strong>Zhiyuan Zhou</strong>,
              <a href="https://camallen.net">Cameron Allen</a>,
              <a href="https://cs.brown.edu/people/kasadiat/authors/kavosh-asadi/">Kavosh Asadi</a>,
              <a href="https://cs.brown.edu/people/gdk/">George Konidaris</a>,
              <br>
				<em>5th Multidisciplinary Conference on Reinforcement Learning and Decision Making (RLDM)</em>, 2022.
              <br>
              <a href="https://arxiv.org/abs/2205.05588">Arxiv</a>
              /
              <a href="">poster</a>
              <br>
              <p>Introduces a way of evaluating action generalization and evaluate DQN's ability to generalize over actions. <br>
              </p>
            </td>
          </tr>
        </tbody>
</table> 

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <!-- <tr bgcolor="#ffffd0"> -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div>
                <img src='/images/paper-images/optre.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <p style="font-family:'Lato',Verdana,Helvetica,sans-serif; font-size:14px;font-weight:700">
              Designing Rewards for Fast Learning
              </p>
              <a href="https://www.linkedin.com/in/henry-sowerby-a54aa592/">Henry Sowerby</a>,
              <strong>Zhiyuan Zhou</strong>,
              <a href="https://www.littmania.com">Michael Littman</a>,
              <br>
				<em>5th Multidisciplinary Conference on Reinforcement Learning and Decision Making (RLDM)</em>, 2022.
              <br>
              <a href="https://arxiv.org/abs/2205.15400">Arxiv</a>
              /
              <a href="">poster</a>
              <br>
              <p>Identifies properties of rewards that lead to fast learning, and proposes an algorithm to design those rewards.<br>
              </p>
            </td>
          </tr>
        </tbody>
</table> 

### School Journal
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <!-- <tr bgcolor="#ffffd0"> -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div>
                <img src='https://www.webtunix.ai/static/img/anotation.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <p style="font-family:'Lato',Verdana,Helvetica,sans-serif; font-size:14px;font-weight:700">
              Improving Post-Processing on Video Object Recognition Using Initial Measurement Unit
              </p>
              <strong>Zhiyuan Zhou</strong>,
              <a href="https://rocketreach.co/spencer-boyum-email_78257776">Spencer Boyum</a>,
              <a href="https://vivo.brown.edu/display/mparadis">Michael Paradiso</a>,
              <br>
				<em>Brown Undergraduate Research Journal</em>, Spring 2022.
              <br>
              <a href="https://brownresearchclub.weebly.com/spring-2022.html">paper</a>
              <br>
              <p>Improving the quality of object recognition on video using spatial and temporal data from Inertial Measurement Unit and Kalman Filter models.<br>
              </p>
            </td>
          </tr>
        </tbody>
</table> 


## Misc 
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <!-- <tr bgcolor="#ffffd0"> -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div>
                <img src='/images/paper-images/robot-nav.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <p style="font-family:'Lato',Verdana,Helvetica,sans-serif; font-size:14px;font-weight:700">
              Natural Language and Gesture Control for Robot Navigation
              </p>
              <a href="https://www.linkedin.com/in/ronald-baker-a978801b4/">Ronald Baker</a>,
              <strong>Zhiyuan Zhou</strong>,
              <a href="https://cs.brown.edu/people/stellex/">Stefanie Tellex</a>,
              <br>
				<em>final paper for Brown's Collaborative Robotics CS2951K</em>, Spring 2020.
              <br>
              <a href="">paper</a>
              /
              <a href="">demo video</a>
              <br>
              <p>Created a pipelien that enables a robot to navigate to a destination more accurately, using pointing gestures to corroborate natural language commands.<br>
              </p>
            </td>
          </tr>
        </tbody>
</table> 