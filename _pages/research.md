---
title: "Research"
layout: splash
permalink: /research/
---
I am broadly interested in building human level intelligence. I believe that Reinforcement Learning (RL) provides a powerful framework for building AI systems, and reward is a good enough incentive to induce approximations of intelligence. To that end, I work on **hierarchical RL, transfer RL, generalization, and reward design**. I am fortunate to be advised by professors [George Konidaris](https://cs.brown.edu/people/gdk/) and [Michael Littman](https://www.littmania.com) under Brown's [BigAI initiative](http://bigai.cs.brown.edu). Feel free to check out some of my work below. 
{: style="text-align: center;font-size:110%;padding-top:40px"}

<!-- style -->
<link rel="stylesheet" href="/assets/css/styles.css">


## Publications
### Conferences 
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <!-- <tr bgcolor="#ffffd0"> -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div>
                <img src='/images/paper-images/actgen.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <p style="font-family:'Lato',Verdana,Helvetica,sans-serif; font-size:14px;font-weight:700">
              Characterizing the Action Generalization Gap in Deep Q-Learning
              </p>
              <div class="skills">
                <span class="skill">DQN</span>
                <span class="skill">action generalization</span>
              </div>
              <strong>Zhiyuan Zhou</strong>,
              <a href="https://camallen.net">Cameron Allen</a>,
              <a href="https://cs.brown.edu/people/kasadiat/authors/kavosh-asadi/">Kavosh Asadi</a>,
              <a href="https://cs.brown.edu/people/gdk/">George Konidaris</a>
              <br>
				<em>5th Multidisciplinary Conference on Reinforcement Learning and Decision Making (RLDM)</em>, 2022.
              <br>
              [<a href="https://arxiv.org/abs/2205.05588">arXiv</a>]
              [<a href="../pdfs/posters/actgen_rldm_poster.pdf">poster</a>]
              [<a href="https://github.com/camall3n/actgen">code</a>]
              <br>
              <p>Introduces a way of evaluating action generalization in Deep Q-Learning using an oracle, and shows that DQN's ability to generalize over actions depends on the size of the action space. <br>
              </p>
            </td>
          </tr>
        </tbody>
</table> 

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <!-- <tr bgcolor="#ffffd0"> -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div>
                <img src='/images/paper-images/optre.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <p style="font-family:'Lato',Verdana,Helvetica,sans-serif; font-size:14px;font-weight:700">
              Designing Rewards for Fast Learning
              </p>
              <div class="skills">
                <span class="skill">reward design</span>
                <span class="skill">Interactive RL</span>
              </div>
              <a href="https://www.linkedin.com/in/henry-sowerby-a54aa592/">Henry Sowerby</a>,
              <strong>Zhiyuan Zhou</strong>,
              <a href="https://www.littmania.com">Michael Littman</a>
              <br>
				<em>5th Multidisciplinary Conference on Reinforcement Learning and Decision Making (RLDM)</em>, 2022.
              <br>
              [<a href="https://arxiv.org/abs/2205.15400">arXiv</a>]
              [<a href="../pdfs/posters/optre_rldm_poster.pdf">poster</a>]
              [<a href="https://brown.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=7adfa2ab-3dde-46ab-b69e-aea800efe5ef">talk at RLDM</a> at 1:20:00]
              <br>
              <p>Identifies properties of rewards that lead to fast learning, and proposes an algorithm to design those rewards. Rewards should have big action gaps and small "subjective discounts".<br>
              </p>
            </td>
          </tr>
        </tbody>
</table> 

### School Journal
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <!-- <tr bgcolor="#ffffd0"> -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div>
                <img src='https://www.webtunix.ai/static/img/anotation.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <p style="font-family:'Lato',Verdana,Helvetica,sans-serif; font-size:14px;font-weight:700">
              Improving Post-Processing on Video Object Recognition Using Initial Measurement Unit
              </p>
                <div class="skills">
                  <span class="skill">object recognition</span>
                  <span class="skill">Hidden Markov Models</span>
                  <span class="skill">Kalman Filter</span>
                  <span class="skill">Inertial Measurement Unit</span>
                </div>
              <strong>Zhiyuan Zhou</strong>,
              <a href="https://rocketreach.co/spencer-boyum-email_78257776">Spencer Boyum</a>,
              <a href="https://vivo.brown.edu/display/mparadis">Michael Paradiso</a>
              <br>
				<em>Brown Undergraduate Research Journal</em>, Spring 2022. (Work was done in Summer 2020, published in 2022.)
              <br>
              [<a href="https://brownresearchclub.weebly.com/spring-2022.html">paper</a> on page 29]
              <br>
              <p>Proposes Kalman Filter models to improving the quality of object recognition on videos using spatial and temporal data from Inertial Measurement Unit.<br>
              </p>
            </td>
          </tr>
        </tbody>
</table> 


## Misc 
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <!-- <tr bgcolor="#ffffd0"> -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div>
                <img src='/images/paper-images/robot-nav.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <p style="font-family:'Lato',Verdana,Helvetica,sans-serif; font-size:14px;font-weight:700">
              Natural Language and Gesture Control for Robot Navigation
              </p>
              <div class="skills">
                <span class="skill">NLP</span>
                <span class="skill">gesture control</span>
                <span class="skill">navigation</span>
                <span class="skill">robotics simulation</span>
              </div>
              <a href="https://www.linkedin.com/in/ronald-baker-a978801b4/">Ronald Baker</a>,
              <strong>Zhiyuan Zhou</strong>,
              <a href="https://cs.brown.edu/people/stellex/">Stefanie Tellex</a>,
              <br>
				<em>final paper for Brown's Collaborative Robotics CS2951K</em>, Spring 2020.
              <br>
              [<a href="../pdfs/papers/nl-gesture-paper.pdf">paper</a>]
              [<a href="https://drive.google.com/file/d/1n_2syscPwRcwtKXXp40UMu6Vl1x3uDok/view?usp=sharing">demo video</a>]
              <br>
              <p>Created a pipeline that enables a robot to navigate to a destination more accurately, using pointing gestures to corroborate natural language commands.<br>
              </p>
            </td>
          </tr>
        </tbody>
</table> 